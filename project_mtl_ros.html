<head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script defer src="theme.js"></script>
    <link rel="stylesheet" href="project.css" />

    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&display=swap" rel="stylesheet" />
</head>

<body>
    <nav class="navbar">
        <ul class="navbar-nav">
            <li class="logo">
                <img src="./assets/neural_net.png" alt="Project 2" class="project-logo" />
            </li>


        </ul>
    </nav>


    <main>
        <div class="project-link-container">
            <p class="sub-title">
                <time datetime="2022-11-01">November 1, 2022</time>
            </p>
            <a href="https://github.com/slabban/ros_pytorch_hydranet/tree/main/ros2_pytorch">
                <img src="./assets/github.png" alt="Repository" class="icon" />
            </a>
        </div>

        <h1 class="title">Hydranets in ROS2</h1>


        <section>
            <p>
                My interest in deep learning had grown throughout my engineering consulting career at Accenture. I was
                quickly picking up software engineering skills through my work on perception and automotive middleware.
                Around the same time, deep learning established its place in robotics and started making its way into
                production vehicles.
            </p>
            <p>
                I was particularly drawn to the concept of <strong>Hydranets</strong>, which seemed promising for
                multi-task learning in robotics. I decided to explore how I could apply it to my work, and I'm excited
                to share my experience with <strong>Deep Learning</strong>.
            </p>
        </section>

        <section>
            <h2>Hydranets in Action</h2>
            <figure>
                <a href="https://www.youtube.com/watch?v=7NW-bPTts8U" class="video-link">
                    <img src="./images/mtl.png" alt="Video illustration of Hydranet in ROS2 middleware"
                        class="youtube-thumbnail">
                </a>

            </figure>

            <p>
                The video above illustrates depth and semantic segmentation inferences from a Hydranet model deployed in
                ROS2 middleware. The image stream is vehicle playback data that my friends and I collected while driving
                around our university campus.
            </p>
        </section>

        <section>
            <h2>Setting Learning Goals</h2>
            <p>
                Deep learning and AI are rapidly evolving fields with constant research pushing the boundaries. While
                understanding and evaluating research papers is key, deployment and integration can be an even more
                challenging task, especially in robotics where high throughput and real-time performance are critical.
            </p>
            <p>
                To mitigate my FOMO, I set the following goals:
            </p>
            <ul>
                <li>Learn to understand and critically evaluate state-of-the-art deep learning papers.</li>
                <li>Learn to create multi-task learning models and train them.</li>
                <li>Integrate new state-of-the-art models in robotic middleware.</li>
            </ul>
        </section>

        <section>
            <h2>Deep Learning Journey</h2>
            <p>
                My deep learning journey started in the summer of 2021, with two amazing resources: fastai's <a
                    href="https://course.fast.ai/">Practical Deep Learning for Coders</a> and Deep Lizard's <a
                    href="https://youtube.com/playlist?list=PLZbbT5o_s2xq7LwI2y8_QtvuXZedL6tQU">Deep Learning
                    Fundamentals</a> and <a
                    href="https://youtube.com/playlist?list=PLZbbT5o_s2xrfNyHZsM6ufI0iZENK9xgG">PyTorch-Python Deep
                    Learning API</a>. These courses equipped me with the theoretical and practical fundamentals to
                explore deeper into Computer Vision.
            </p>
            <p>
                To specialize further, I enrolled in Jeremy Cohen's <a href="https://courses.thinkautonomous.ai/">Think
                    Autonomous</a> course. It covered theoretical and practical aspects of Hydranets and multi-task
                learning, based on concepts from <a href="https://arxiv.org/abs/1809.04766">this paper</a>.
            </p>
        </section>

        <section>
            <h2>Hydranet Architecture</h2>
            <figure>
                <img src="images/hydranet.png"
                    alt="Diagram of the Hydranet architecture, including encoder and decoder structure"
                    class="project-image">
            </figure>
            <p>
                Much like the well-known <strong>U-Net</strong>, Hydranet follows an encoder-decoder architecture. The
                encoder reduces the spatial dimensions while increasing feature channels, and the decoder restores
                spatial dimensions with skip connections from the encoder. This enables the model to retain spatial
                features while leveraging the encoderâ€™s feature extraction power.
            </p>
            <p>
                For real-time applications, the authors of the paper focused on efficiency, using <strong>Mobilenet
                    V2</strong> as the encoder and a lightweight <strong>RefineNet</strong> as the decoder. They also
                incorporated depthwise separable convolutions to reduce the number of parameters and floating-point
                operations.
            </p>
        </section>

        <section>
            <h2>Integrating Hydranet into ROS2</h2>
            <p>
                For those new to robotics, <strong>ROS2</strong> (Robot Operating System 2) is an open-source middleware
                that enables roboticists to create reusable, flexible code for a variety of robot applications.
                Middleware facilitates communication between different processes and devices, enabling data sharing.
            </p>
            <p>
                My goal was to integrate the Hydranet model into ROS2, learning both ROS2 and Deep Learning
                simultaneously. The integration journey involved learning about <strong>PyTorch JIT</strong> for tracing
                models and <strong>Libtorch</strong> for C++ API integration. This enabled me to take a trained model
                from eager mode to production mode.
            </p>
            <p>
                I deployed the model in a ROS2 node, processing camera images and passing them through the network for
                inference, producing depth and segmentation outputs.
            </p>
        </section>

    </main>



    <footer>
        <nav class="footer-nav">
            <div class="nav-links-container">
                <ul class="nav-links">
                    <li><a href="./index.html">About Me</a></li>
                </ul>
            </div>
        </nav>
        <p id="copyright">Copyright &#169; 2023 Samer Labban. All Rights Reserved.</p>
    </footer>

</body>