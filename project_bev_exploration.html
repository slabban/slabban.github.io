<head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script defer src="theme.js"></script>
    <link rel="stylesheet" href="project.css" />
    <link rel="stylesheet" href="bev_exploration.css" />


    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&display=swap" rel="stylesheet" />
</head>

<body>
    <nav class="navbar">
        <ul class="navbar-nav">
            <li class="logo">
                <img src="./assets/birds_eye_view.png" alt="Project 1" class="project-logo" />
            </li>


        </ul>
    </nav>


    <main>
        <div class="project-link-container">
            <p class="sub-title">
                <time datetime="2023-03-15">March 15, 2023</time>
            </p>
            <a href="https://github.com/slabban/mindmaps">
                <img src="./assets/github.png" alt="Repository" class="icon" />
            </a>
        </div>

        <h1 class="title">A Perspective on Birds Eye View (BEV) Networks</h1>


        <section>
            <h2>Why BEV Networks?</h2>
            <p>If you’ve been following advancements in 3D perception for autonomous driving, you’ve likely come across
                Bird’s Eye View (BEV) networks. But what are they? There are many different approaches these days, but
                to put it simply, a BEV network is a type of deep
                learning model that takes in sensor input such as images form a camera and outputs a representation of
                the
                scene from a top-down perspective. They’re becoming a go-to method for enhancing how self-driving cars
                “see” the world. </p>

            <img src="./images/bev_lps.gif" alt="Lift-Splat-Shoot Process" class="project-image" />

            <p>Shown above is one of the pioneering papers in this space, <a
                    href="https://github.com/nv-tlabs/lift-splat-shoot">Lift-Splat-Shoot</a>. Using inputs from six
                cameras of around the driving vehicle, this BEV network
                is able to generate a single unified representation of the surrounding vehicles.</p>

            <p>What makes BEV so attractive? It handles three crucial symmetries:</p>
            <ul>
                <li><strong>Translation Equivariance</strong>: Shift an image, and the output shifts along with it.
                    Fully convolutional single-
                    image object detectors roughly have this property and the BEV Nets inherits this from them.</li>
                <li><strong>Permutation Invariance</strong>: The camera order doesn’t matter; the results are the same.
                </li>
                <li><strong>Ego-Frame Isometry Equivariance</strong>: No matter where the camera is placed, it’ll detect
                    the same objects in the ego frame.</li>
            </ul>
            <p>By preserving these properties and being fully end-to-end differentiable, BEV networks streamline the
                perception process.</p>

        </section>

        <section>
            <h2>Understanding The BEV Landscape</h2>

            <p>Here’s the thing—I don’t have a PhD from Carnegie Mellon or MIT. For a long time, I thought breaking into
                such a complex field without one was intimidating. But as I progressed in my career and became more
                established in the automated driving industry, I realized something: being passionate about learning and
                sharing that journey has been my greatest asset. With this project, my goal is to show everyone,
                regardless of skill level, my unique perspective on BEV with the hope to inspire others to do the same.
            </p>


            <img src="./images/bev_mindmap.png" alt="bev lift strategies" class="project-image" />

            <p>Above is a snapshot into my BEV mind map, created with <a href="https://obsidian.md/">Obsidian</a>. Using
                this app, I was able to gradually expand my knowledge in
                BEV networks, starting with the <i>depth-based</i> <a href="https://arxiv.org/abs/2104.10490">Fiery</a>
                and eventually going across the main approaches discussed in this project.
            </p>
        </section>

        <section>
            <h2>Datasets That Power BEV</h2>
            <img src="./images/BEV_Datasets.png" alt="bev lift strategies" class="project-image" />

            <p>To get BEV networks to work, we rely on some heavyweight datasets offering diverse scenes for training
                and testing. Three of the big names are:</p>
            <ul>
                <li><strong>KITTI</strong>: A well-known benchmark with over 7,500 samples for testing with both 2D and
                    3D annotations.</li>
                <li><strong>nuScenes</strong>: A massive dataset with 1,000 scenes, each lasting 20 seconds, offering a
                    360-degree field of view.</li>
                <li><strong>Waymo Open Dataset (WOD)</strong>: One of the largest autonomous driving datasets with
                    nearly 800 sequences for training.</li>
            </ul>
            <p>Other datasets worth mentioning include <strong>Argoverse</strong>, <strong>H3D</strong>, and
                <strong>Lyft L5</strong>, which are also popular for testing BEV models.
            </p>
        </section>

        <section>
            <h2>Lifting Strategies</h2>

            <p>A <i>lifting strategy</i> is the process of transforming 2D image features from a perspective view (PV)
                into a bird's eye view (BEV) representation. This can be thought of as the core mechanism of BEV
                methodologies.</p>

            <img src="./images/lift_strategies.png" alt="bev lift strategies" class="project-image" />

            <p>Shown above is an illustration of popular approaches to transforming perspective view (PV)
                images into bird's eye view (BEV) representations. The four quadrants demonstrate (clockwise from
                top-left):
            <table class="bev-table">
                <tr>
                    <th>Category</th>
                    <th>Architecture</th>

                </tr>
                <tr>
                    <td>MLP-Based</td>
                    <td><a href="https://arxiv.org/abs/2003.13402">Pyramid Occupancy Networks</a> </td>

                </tr>
                <tr>
                    <td>Depth-Based</td>
                    <td><a href="https://github.com/nv-tlabs/lift-splat-shoot">Lift-Splat-Shoot</a></td>
                </tr>
                <tr>
                    <td>Transformer-Based (Scanline)</td>
                    <td><a href="https://arxiv.org/abs/2110.00966">Image2Map</a></td>
                </tr>
                <tr>
                    <td>Transformer-Based (Deformable Attention)</td>
                    <td><a href="https://arxiv.org/abs/2203.17270">BEVFormer</a></td>
                </tr>
            </table>
            </p>

            <p>It's a good idea to establish common ground before diving into each approach. Each approach uses what is
                called a <strong>backbone</strong>. This backbone encoder is generally a fully convolutional neural
                network (CNN) that takes in an image and outputs
                a
                feature map that are passed to the lifting strategies. Once these features are translated into the
                unified BEV space, the network may then
                feed these BEV features into a single, or set of task heads to produce the final predictions.
            </p>


            <p>Let's now focus on the lifting strategies:</p>

            <h3>Geometric-Based PV2BEV</h3>
            <ul>
                <li><strong>Homography-Based</strong>: Also known as <strong>Invese Perspective Mapping</strong>. Most
                    applications of this actually don't use neural networks, but instead applies simple
                    geometric transformations on the sensor intrinsics and extrinsics to get a BEV-like view. This falls
                    in the traditional realm of Robotics algorithms and has been used extensively in perception
                    pipelines. However, this technique falls short due to the assumption that the ground is flat, which
                    can Lead
                    to poor results
                </li>
                <li><strong>Depth-Based</strong>: The networks rely on geometric priors such sensor intrinsics and
                    extrinsics to extract depth from
                    image features, enabling them to project 2D images into 3D space and then to a BEV plane.
                    Starting
                    in
                    2019 with single image detection, depth networks had seen a major boom. Early approaches focused on
                    a <a href="https://arxiv.org/pdf/1812.07179">pseudo-LiDAR</a> strategy, which extracts a depth map
                    and the back-projects this map into a 3D point-cloud; however, voxel-based approaches, such as <a
                        href="https://github.com/nv-tlabs/lift-splat-shoot">Lift-Splat-Shoot</a> and <a
                        href="https://arxiv.org/pdf/2104.10490">Fiery</a>, took the lead due to their reduced complexity
                    and flexbilty.
                </li>
            </ul>


            <h3>Network-Based PV2BEV</h3>
            <ul>
                <li><strong>MLP-Based</strong>: These models use multilayer perceptrons (MLPs) to map PV images into BEV
                    space; ignoring the geometric priors of calibrated cameras. This avenue was not explored deeply
                    since the <i>Transformer-Based</i> methods have generally outperformed them, but notable works such
                    as <a href="https://arxiv.org/abs/2003.13402">Pyramid Occupancy Networks</a> (Shown Above) and <a
                        href="https://arxiv.org/abs/2006.09917">FishingNet</a> are notable examples.</li>
                <li><strong>Transformer-Based</strong>: <a href="https://arxiv.org/abs/1706.03762">Transformers</a> made
                    their name in the <i>Natural Language
                        Processing</i> scene and are the cornerstone of the AI boom at the end of 2022. What's amazing
                    is that adapting them to learn BEV representations is conceptually straightforward, and they work
                    <i>really well</i>. Not only are they an effective lifting strategy, but transformers can also be
                    switched out in place of backbone encoders or one of the task head(s) that the network is
                    predicting, which can lead to improved accuracy and reduced model complexity.

                    <br /><br />

                    Transformer architectures may fall into two categories: <i>sparse</i> and <i>dense</i> query-based.
                    Inspired by <a href="https://arxiv.org/abs/1706.03762">DETR</a>, <i>sparse query-based</i> methods
                    directly produce perception results
                    and avoid the dense transformation of image features. These methods excel at detection tasks such as
                    3D
                    object detection, but fall short at segmentation tasks.

                    On the other hand, <i>dense query-based</i> methods, such as <a
                        href="https://arxiv.org/abs/2203.17270">BEVFormer</a> (shown above), methods are designed to
                    learn BEV
                    representations my mapping the queries into a pre-defined 3D volumetric or BEV space, which can be
                    ported to perception tasks such as 3D object detection and segmentation; however, these models
                    suffer in computational performance due to the dense queries which has lead to more research into
                    efficient transformer architectures.

                </li>
            </ul>
        </section>

        <section>
            <h2>How Do We Compare These Methods?</h2>
            <p>Comparing BEV methods isn’t easy. Each approach has its strengths and weaknesses. That’s where
                <strong>Simple-BEV</strong> comes in—a benchmark designed to level the playing field for various BEV
                methodologies. By simplifying the comparison, it helps researchers evaluate models more fairly.
            </p>
        </section>

        <section>
            <h2>Multi-Task Learning with BEV Networks</h2>
            <p>BEV networks often deal with multiple tasks like object detection and segmentation. Multi-task learning
                is a popular way to handle this, and one method that stands out is <strong>Uncertainty
                    Weighting</strong>, where tasks are prioritized dynamically based on their difficulty or importance.
            </p>
            <p>Other methods include <strong>GradNorm</strong> and <strong>Meta-Learning Task Weighting</strong>, which
                adjust task weights to balance performance across different objectives.</p>
        </section>

    </main>



    <footer>
        <nav class="footer-nav">
            <div class="nav-links-container">
                <ul class="nav-links">
                    <li><a href="./index.html">About Me</a></li>
                </ul>
            </div>
        </nav>
        <p id="copyright">Copyright &#169; 2023 Samer Labban. All Rights Reserved.</p>
    </footer>

</body>