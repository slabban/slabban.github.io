<head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script defer src="theme.js"></script>
    <link rel="stylesheet" href="project.css" />
    <link rel="stylesheet" href="bev_exploration.css" />


    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&display=swap" rel="stylesheet" />
</head>

<body>
    <nav class="navbar">
        <ul class="navbar-nav">
            <li class="logo">
                <img src="./assets/bev_logo.png" alt="Project 1" class="project-logo" />
            </li>


        </ul>
    </nav>


    <main>
        <div class="project-link-container">
            <p class="sub-title">
                <time datetime="2024-10-28">October 28, 2024</time>
            </p>
            <a href="https://github.com/slabban/mindmaps">
                <img src="./assets/github.png" alt="Repository" class="icon" />
            </a>
        </div>

        <h1 class="title">A Perspective on Birds Eye View (BEV) Networks</h1>


        <section>

            <p>If you’ve been following advancements in 3D perception for autonomous driving, you’ve likely come across
                Bird’s Eye View (BEV) networks. But what are they? There are many different approaches these days, but
                to put it simply, a BEV network is a type of deep learning model that takes in sensor input, such as
                images from a camera, and outputs a representation of the scene from a top-down perspective. They’re
                becoming a go-to method for enhancing how self-driving cars
                “see” the world. </p>

            <figure>
                <img src="./images/bev_lps.gif" alt="Lift-Splat-Shoot Process" class="project-image" />
            </figure>

            <p>Shown above is one of the pioneering papers in this space, <a
                    href="https://arxiv.org/abs/2008.05711">Lift-Splat-Shoot</a>. Using inputs from six
                cameras of around the driving vehicle, this BEV network
                is able to generate a single unified representation of the surrounding vehicles.</p>


            <figure>
                <img src="./images/bev_mindmap.png" alt="bev lift strategies" class="mindmap-image" />
            </figure>

            <p>Above was my personal journey exploring the BEV research space, created with <a
                    href="https://obsidian.md/">Obsidian</a>. In this article we explore such BEV applications, their
                architectures, and how they fit in modern
                and upcoming autonomous stacks.</p>

        </section>

        <section>
            <h2>Motivation to Understand The BEV Landscape</h2>

            <h3>Evolution of the Autonomous Vehicle Stack</h3>
            <p>
                Landing on the scene in the late 1980s, classical methods like Canny edge detection and
                homography-based techniques were key
                components of early autonomous driving systems, helping
                to identify lane boundaries and transform camera perspectives for better road understanding. Over time,
                these classical techniques were modularly replaced by deep learning-based systems, as modern CNN-based
                object detection and segmentation networks showed promising results in the early perception pipelines
                and naturally evolved into to the state of the art Birds Eye View networks, which offer more
                accurate and robust top-down scene interpretation directly from multi-sensor data.
            </p>

            <p>In their paper <a href="https://arxiv.org/abs/2108.05805">Reimagining an Autonomus Vehicle</a>,
                researchers from <a href="https://wayve.ai/">Wayve</a> show an
                indicative vision of where traditional rule-based autonomy stacks and their evolution to
                end-to-end architectures.
            </p>

            <figure>
                <img src="./images/wayve_av1.png" alt="Wayve Autonomous Vehicle" class="project-image" />
            </figure>


            <p>Shown above is what is considered the modern or traditional autonomous stack, consisting of Perception,
                Planning, and Control. Which Wayve terms <strong>AV1.0</strong>.

                The perception pipeline takes the raw sensor input from the various sources, and feeds them through
                various
                scene understanding pipelines, which are then fed to planning pipelines to assemble into path-planning
                tasks and are sent to controls for actuation commands.

                Wayve makes their case that these traditional hand-coded architectures have merit, but are ultimately
                brittle and prone to flaws and edge case gaps when faced with the goal of achieving true autonomous
                driving.
            </p>

            <figure>
                <img src="./images/wayve_av2.png" alt="Wayve Autonomous Vehicle" class="project-image" />
            </figure>

            <p> This <strong>AV2.0</strong> architecture takes the processed sensor input directly into an end to end
                network to
                generate a motion plan for the controller to follow.

                Wayve proposes this architecture, positing that the autonomous driving future will-be be
                completely data-driven, and that having this system would lend itself to a clean and robust learning
                pipeline that is generalizable to different vehicles.</p>


            <h3>Where BEV fits</h3>

            <p>
                Bird’s Eye View networks can fit into both AV1.0 and AV2.0 stacks, sitting at the
                perception portion of the stack since the nature BEV directly works with multi-sensor
                features.
            </p>

            <figure>
                <img src="./images/bevformer_arch.png" alt="bevformer" class="project-image" />
            </figure>

            <p>
                Above is an example of the original <a href="https://arxiv.org/abs/2203.17270">BEVFormer</a>
                architecture. In this paper, the authors train the network to perform segmentation and 3D object
                detection, which are some outputs of the perception pipeline in AV1.0.
            </p>

            <figure>
                <img src="./images/UniAD.png" alt="UniAD Autonomous Vehicle" class="project-image" />
            </figure>

            <p>
                A powerful illustration of BEV in AV2.0 comes from the winning paper of CVPR 2023, <a
                    href="https://arxiv.org/abs/2212.10156">UniAD</a>,
                focuses on the end-to-end architecture. The authors integrate the BEVFormer as the backbone of their
                End-to-End
                architecture, which
                provides a unified basis for downstream processing.
            </p>


            <p>What makes BEV so attractive? My favorite techincal explanation comes from the <i>lift, splat, shoot</i>
                authors:
            </p>
            <ul>
                <li><strong>Translation Equivariance</strong>: Shift an image, and the output shifts along with it.
                    Fully convolutional single-
                    image object detectors roughly have this property and the BEV Nets inherit this from them.</li>
                <li><strong>Permutation Invariance</strong>: The camera order doesn’t matter; the results are the same.
                </li>
                <li><strong>Ego-Frame Isometry Equivariance</strong>: No matter where the camera is placed, it’ll detect
                    the same objects in the ego frame.</li>
            </ul>
            <p>By preserving these properties and being fully end-to-end differentiable, BEV networks streamline the
                perception process.</p>
        </section>



        <section>
            <h2>Lifting Strategies</h2>

            <h3>Overview</h3>

            <p>The <i>lifting strategy</i> is the process of transforming 2D image features and other sensing inputs
                from a perspective view (PV)
                into a bird's eye view (BEV) representation. These strategies can be thought of as the core mechanism of
                BEV
                methodologies.</p>

            <figure>
                <img src="./images/lift_strategies.png" alt="bev lift strategies" class="project-image" />
            </figure>

            <p>Shown above is an illustration of popular approaches to transforming perspective view (PV)
                images into bird's eye view (BEV) representations. The four quadrants demonstrate (clockwise from
                top-left):
            <table class="bev-table">
                <tr>
                    <th>Category</th>
                    <th>Architecture</th>

                </tr>
                <tr>
                    <td>MLP-Based</td>
                    <td><a href="https://arxiv.org/abs/2003.13402">Pyramid Occupancy Networks</a> </td>

                </tr>
                <tr>
                    <td>Depth-Based</td>
                    <td><a href="https://github.com/nv-tlabs/lift-splat-shoot">Lift-Splat-Shoot</a></td>
                </tr>
                <tr>
                    <td>Transformer-Based (Scanline)</td>
                    <td><a href="https://arxiv.org/abs/2110.00966">Image2Map</a></td>
                </tr>
                <tr>
                    <td>Transformer-Based (Deformable Attention)</td>
                    <td><a href="https://arxiv.org/abs/2203.17270">BEVFormer</a></td>
                </tr>
            </table>
            </p>

            <p>It's a good idea to establish the flow before diving into the details. Each approach uses what is
                called a <strong>backbone</strong>. This backbone, or encoder, is generally a fully convolutional neural
                network (CNN) that takes in an image and outputs
                a
                feature map that are passed to the lifting strategies. Once these features are translated into the
                unified BEV space, the network may then
                feed these BEV features into a single, or set of task heads to produce the final predictions.
            </p>

            <h3>Strategies Breakdown</h3>
            <figure>
                <img src="./images/bev_graph.png" alt="bev graph" class="project-image" />
            </figure>

            <p>I really like how this <a href="https://arxiv.org/pdf/2208.02797">survey paper</a> breaks it
                down. Following their topology, we will go through the big four fom left to right.
            </p>

            <h3>Geometric-Based PV2BEV</h3>
            <ul>
                <li><strong>Homography-Based</strong>: Also known as <strong>Inverse Perspective Mapping</strong>. This
                    concept has actually existed since the 90s, and applications of this traditionally haven't used
                    neural networks. Homography applies simple geometric transformations to the sensor intrinsics and
                    extrinsics to get a BEV-like view. This falls within the traditional realm of robotics algorithms
                    and
                    has been extensively used in perception pipelines. However, this technique falls short due to
                    the
                    assumption that the ground is flat, which can be inadequate in more demanding automated driving
                    applications.
                </li>
                <li><strong>Depth-Based</strong>: These networks use geometric priors such sensor intrinsics and
                    extrinsics to extract depth from
                    image features using deep learning techniques, enabling them to project sensor data into 3D space
                    and then to a BEV plane. Starting in 2019 with single image detection, depth networks have seen
                    significant adoption in the industry. Early approaches focused on
                    a <a href="https://arxiv.org/pdf/1812.07179">pseudo-LiDAR</a> strategy, which extracts a depth map
                    and then back-projects this map into a 3D point-cloud; however, voxel-based approaches, such as <a
                        href="https://github.com/nv-tlabs/lift-splat-shoot">Lift-Splat-Shoot</a> and <a
                        href="https://arxiv.org/pdf/2104.10490">Fiery</a>, took the lead due to their reduced complexity
                    and flexibility.
                </li>
            </ul>


            <h3>Network-Based PV2BEV</h3>
            <ul>
                <li><strong>MLP-Based</strong>: These models use multilayer perceptrons (MLPs) to map PV images into BEV
                    space; ignoring the geometric priors of calibrated cameras. This avenue was not explored deeply
                    since the <i>Transformer-Based</i> methods have generally outperformed them, but notable works such
                    as <a href="https://arxiv.org/abs/2003.13402">Pyramid Occupancy Networks</a> (Shown Above) and <a
                        href="https://arxiv.org/abs/2006.09917">FishingNet</a> are notable examples.</li>
                <li><strong>Transformer-Based</strong>: <a href="https://arxiv.org/abs/1706.03762">Transformers</a> made
                    their name in the <i>Natural Language
                        Processing</i> scene and are the cornerstone of the AI boom at the end of 2022. What's amazing
                    is that adapting them to learn BEV representations is conceptually straightforward, and they work
                    <i>really well</i>. Not only are they an effective lifting strategy, but transformers can also be
                    switched out in place of backbone encoders or one of the task head(s) that the network is
                    predicting, which can lead to improved accuracy and reduced model complexity.

                    <br /><br />

                    Transformer architectures may fall into two categories: <i>sparse</i> and <i>dense</i> query-based.
                    Inspired by <a href="https://arxiv.org/abs/1706.03762">DETR</a>, <i>sparse query-based</i> methods
                    directly produce perception results
                    and avoid the dense transformation of image features. These methods excel at detection tasks such as
                    3D
                    object detection, but fall short at segmentation tasks.

                    On the other hand, <i>dense query-based</i> methods, such as <a
                        href="https://arxiv.org/abs/2203.17270">BEVFormer</a>, methods are designed to
                    learn BEV
                    representations by mapping the queries into a pre-defined 3D volumetric or BEV space, which can be
                    ported to perception tasks such as 3D object detection and segmentation; however, these models
                    suffer in computational performance due to the dense queries which has inspired more research into
                    efficient transformer architectures. A notable approach is
                    <a href="https://arxiv.org/abs/2110.00966">Image2Map</a>, which assumes a 1:1 relationship between
                    vertical scanlines of an image and the camera rays in the BEV space, which avoids the dense query
                    problem.

                </li>
            </ul>
        </section>

        <section>
            <h2>Benchmarking BEV for Perception</h2>

            <figure>
                <img src="./images/BEV_Datasets.png" alt="bev lift strategies" class="project-image" />
            </figure>

            <p> Datasets offering diverse scenes for
                training
                and testing are crucial for researchers to benchmark BEV models. Three of the big names are:</p>
            <ul>
                <li><strong><a
                            href="https://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d">KITTI</a></strong>:
                    A well-known benchmark with over 7,500 samples for testing with both 2D
                    and
                    3D annotations.</li>
                <li><strong><a href="https://www.nuscenes.org/nuscenes">nuScenes</a></strong>: A massive dataset with
                    1,000 scenes, each lasting 20 seconds,
                    offering a
                    360-degree field of view.</li>
                <li><strong><a href="https://waymo.com/open/">Waymo Open Dataset (WOD)</a></strong>: One of the largest
                    autonomous driving datasets with
                    nearly 800 sequences for training.</li>
            </ul>
            <p>Other datasets worth mentioning include <strong>Argoverse</strong>, <strong>H3D</strong>, and
                <strong>Lyft L5</strong>, which are also popular for testing BEV models.
            </p>

            <p> For perception evaluation metrics, the most commonly
                used criterion for BEV Detection is <strong>Average Precision (AP)</strong>
                and the <strong>mean Average Precision (mAP)</strong> over different
                classes
                or difficulty levels. For BEV Segmentation, <strong>Intersection over Union (IoU)</strong> is used for
                each class
                and mIoU over all classes are frequently used as the metrics.</p>
        </section>

        <section>
            <h2>Final Thoughts</h2>
            <p>
                Following the timeline of the evolution of autonomous driving, we can see a clear pattern
                of classical rule-based techniques being replaced with network-based approaches. New research
                explores how to
                expand these networks to handle the entire end-to-end, from sensor input to planning of the automated
                vehicle.


                <br /><br />

                <strong>What does this mean for the automotive industry and software engineering teams that
                    have been developing perception, localization, and planning pipelines using rule-based
                    techniques?</strong>


                <br /><br />

                I don't have a magic crystal ball, but after being on the field as an ADAS engineer for a few years, I
                can give an informed perspective.

                <br /><br />
                Modern network-based
                architectures will continue to grow and
                dominate cutting-edge autonomous systems that we see from companies like <a
                    href="https://wayve.ai/">Wayve</a> and <a href="https://waymo.com/">Waymo</a>, but traditional
                algorithms still play a vital role
                due to their
                compatibility & efficiency in resource-constrained systems that the majority of current cars use. It's
                likely that the roadmap to end-to-end adoption will be a slow burn that will naturally build as
                vehicle hardware architectures evolve. Organizational shifts will also need time to transition from
                rule-based development to network-based applications, which will drive upskill from traditional
                software development
                methodologies to the AI centric data-driven paradigm.


                <br /><br />

                In my view, the real challenge lies in addressing the external factors. Regulatory approval for
                autonomous driving has evolved from minimal oversight during early experimental testing to rigorous
                safety and explainability requirements, now focusing on functional safety, liability, and cybersecurity.
                As AV technology matures, international regulations emphasize level-specific testing, data-driven
                validation, and public trust to ensure safe deployment.

                <br /><br />
                Perhaps the most important factor is the customer. Autonomous driving is
                ultimately a feature of the vehicle, not the sole purpose of its design. While the allure of
                cutting-edge, fully autonomous systems is undeniable, automakers must strike a balance between
                technological advancements and the practical benefits they deliver to the consumer. It’s essential for
                automakers to focus on features that provide real value to drivers, rather than forcing innovations that
                may not align with consumer desires.
            </p>
        </section>


    </main>



    <footer>
        <nav class="footer-nav">
            <div class="nav-links-container">
                <ul class="nav-links">
                    <li>
                        <a href="./index.html">
                            <img src="./assets/home_1.png" alt="Home icon" class="footer-icon" />
                        </a>
                    </li>
                    <li>
                        <div class="footer-icons-container">
                            <a href="https://github.com/slabban">
                                <img src="./assets/github.png" alt="Github icon" class="footer-icon" />
                            </a>
                            <a href="mailto:slabban2@gmail.com">
                                <img src="./assets/email_1.png" alt="Email icon" class="footer-icon" />
                            </a>
                            <a href="https://www.linkedin.com/in/samer-labban-7b932372">
                                <img src="./assets/linkedin.png" alt="LinkedIn icon" class="footer-icon" />
                            </a>
                        </div>
                    </li>
                </ul>
            </div>
        </nav>
        <p id="copyright">Copyright &#169; 2023 Samer Labban. All Rights Reserved.</p>
    </footer>

</body>